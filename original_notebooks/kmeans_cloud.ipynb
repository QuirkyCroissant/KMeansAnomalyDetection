{
 "cells": [
  {
   "cell_type": "code",
   "id": "639b875b35bf57f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T21:37:29.379669Z",
     "start_time": "2024-11-09T21:37:28.456822Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml import Pipeline\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql.functions import lit"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T21:37:37.387475Z",
     "start_time": "2024-11-09T21:37:29.380675Z"
    }
   },
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"KMeansSession\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "data_path = \"kddcup.data_10_percent\"\n",
    "raw_data = spark.read.csv(data_path, header=False, inferSchema=True)\n",
    "\n",
    "#print(raw_data.summary())"
   ],
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Users/Merlin/Documents/GitHub/KMeansAnomalyDetection/original_notebooks/kddcup.data_10_percent.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Load data\u001B[39;00m\n\u001B[0;32m      5\u001B[0m data_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkddcup.data_10_percent\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 6\u001B[0m raw_data \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(data_path, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[0;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m    739\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(path)))\n\u001B[0;32m    741\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n\u001B[0;32m    743\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[0;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/Merlin/Documents/GitHub/KMeansAnomalyDetection/original_notebooks/kddcup.data_10_percent."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4e553b1b-4f5a-4a64-806b-40523c9ddbd8",
   "metadata": {},
   "source": [
    "#we assemble a KMeans-capable dataframe from a \"finished\" dataframe we already assembled\n",
    "def assemble_vector(dataframe, columns):\n",
    "    #vec_assembler = VectorAssembler(inputCols=dataframe.columns, outputCol=\"features\")\n",
    "    vec_assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "    return vec_assembler.transform(dataframe)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2028966-1024-4ea1-b98e-c295eb3b594c",
   "metadata": {},
   "source": [
    "#for the first two tasks, we need to drop all non-numeric columns, as kMeans cannot deal with them \n",
    "def is_numeric_column(column):\n",
    "    return column[1] != \"string\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bfcb2b2a-d885-4568-802a-4e089b8b88bf",
   "metadata": {},
   "source": [
    "numeric_columns = []\n",
    "non_numeric_columns = []\n",
    "\n",
    "for column in raw_data.dtypes:\n",
    "    if is_numeric_column(column):\n",
    "        numeric_columns.append(column[0])\n",
    "    else:\n",
    "        non_numeric_columns.append(column[0])\n",
    "\n",
    "#print(numeric_columns)\n",
    "\n",
    "#dataset we use in Tasks 1 and 2\n",
    "#numeric_data = raw_data.drop(*non_numeric_columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0744d9d0-df9a-46b8-a92d-4f1e3aad6fde",
   "metadata": {},
   "source": [
    "#we use this evaluator for all KMeans models\n",
    "#evaluator = ClusteringEvaluator(predictionCol='prediction',\n",
    "#                                featuresCol='features',\n",
    "#                                metricName='silhouette',\n",
    "#                                distanceMeasure='squaredEuclidean') "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inaccurate_label_data = assemble_vector(raw_data, numeric_columns)\n",
    "\n",
    "k_from = 2\n",
    "k_to = 75\n",
    "squared_score = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(k_from, k_to):\n",
    "    kmeans = KMeans(k=i, seed=1)\n",
    "    model = kmeans.fit(inaccurate_label_data)\n",
    "    score = model.summary.trainingCost\n",
    "    squared_score.append(score)\n",
    "    print('Objective Function for k =', i, 'is', score)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Execution: {duration} seconds.\")"
   ],
   "id": "66bd58a39ccb3db8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "744bf154a96ead4e",
   "metadata": {},
   "source": [
    "def scale_dataframe(input_dataframe, start_columns):\n",
    "    \n",
    "    #start_columns = input_dataframe.columns\n",
    "    assembled_col = [col+\"_vec\" for col in start_columns]\n",
    "    scaled_col = [col+\"_scaled\" for col in assembled_col]\n",
    "    assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in start_columns]\n",
    "    scalers = [StandardScaler(inputCol=col, outputCol=col + \"_scaled\") for col in assembled_col]\n",
    "    pipeline = Pipeline(stages=assemblers + scalers)\n",
    "    scalerModel = pipeline.fit(input_dataframe)\n",
    "    scaledData = scalerModel.transform(input_dataframe)\n",
    "    \n",
    "    scaledData = scaledData.drop(*start_columns, *assembled_col)\n",
    "\n",
    "    return scaledData, scaled_col"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def one_code_encode(dataframe, column):\n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=column+\"_indexed\")]\n",
    "    encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(), outputCol= column+'_encoded') for indexer in indexers]\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders], outputCol=column+\"_protocol\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "    model = pipeline.fit(dataframe)\n",
    "    transformed = model.transform(dataframe)\n",
    "    return transformed.drop(column+'_indexed', column+'_encoded'), column+\"_protocol\""
   ],
   "id": "9ed16869856d16aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feature_cols = numeric_columns.copy()\n",
    "modified_data, col = one_code_encode(raw_data, column='_c1')\n",
    "feature_cols += [col]\n",
    "#modified_data, col = one_code_encode(modified_data, column='_c2')\n",
    "#feature_cols += [col]\n",
    "#modified_data, col = one_code_encode(modified_data, column='_c3')\n",
    "#feature_cols += [col]\n",
    "modified_scaled_data, scaled_col = scale_dataframe(modified_data, feature_cols)\n",
    "transformed_modified = assemble_vector(modified_scaled_data, scaled_col)\n",
    "#transformed_modified.show()"
   ],
   "id": "1ac34e7776f71a44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# (41, {2: 2.8142, 13: 0.0375, 14: 0.0325, 19: 2.5761, 22: 0.139, 23: 0.0849, 24: 2.4344, 26: 0.2285, 31: 0.0002, 33: 0.165, 39: 2.0554}))]\n",
   "id": "b1a1f97579b238aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k_from_task_4 = 65\n",
    "k_to_task_4 = 66\n",
    "squared_score_task_4 = []\n",
    "predictions = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(k_from_task_4, k_to_task_4)):\n",
    "    kmeans = KMeans(k=i, seed=1)\n",
    "    model = kmeans.fit(transformed_modified)\n",
    "    predictions.append(model.transform(transformed_modified))\n",
    "    score = model.summary.trainingCost\n",
    "    squared_score_task_4.append(score)\n",
    "    #print('Objective Function for k =', i, 'is', score)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time"
   ],
   "id": "7d357fa7d76769b0",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def entropy_score(dataframe):\n",
    "    \n",
    "    x = dataframe \\\n",
    "        .groupBy('prediction') \\\n",
    "        .count() \\\n",
    "        .sort('prediction') \\\n",
    "        .toPandas()\n",
    "    \n",
    "    gamma = dataframe \\\n",
    "        .groupBy('prediction', '_c41') \\\n",
    "        .count() \\\n",
    "        .sort('prediction') \\\n",
    "        .toDF('prediction', 'label', 'count').toPandas()\n",
    "    \n",
    "    total_entropy = 0\n",
    "    for _, rows in x.iterrows():\n",
    "        cluster_id = rows['prediction']\n",
    "        amount_objects = rows['count']\n",
    "        cluster_label_counts = gamma.loc[gamma['prediction'] == cluster_id].values[:, 2].astype(np.float64)\n",
    "        a = np.divide(cluster_label_counts, amount_objects)\n",
    "        cluster_sum = np.sum(np.multiply(a, np.log2(a)))\n",
    "        total_entropy -= cluster_sum * amount_objects / raw_data.count()\n",
    "    \n",
    "    #print('entropy calculated')\n",
    "    return total_entropy"
   ],
   "id": "fad2ee05b0325114",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_time = time.time()\n",
    "entropy_list = [entropy_score(i) for i in tqdm(predictions)]\n",
    "end_time = time.time()\n",
    "duration_eval = end_time - start_time"
   ],
   "id": "fa7d24422d79df91",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"KMeans Execution: {duration} seconds.\")\n",
    "print(f\"Evaluation(Entropy) Execution: {duration_eval} seconds.\")"
   ],
   "id": "bf9b15b3a39a0f8f",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
